{
 "cells": [
  {
   "source": [
    "## Simple Encoder Decoder Architecture for MT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nK4m5HVAXXWr",
    "outputId": "5d6bee8f-87ce-4fdc-f8b4-591cee8a8394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
      "\r",
      "\u001b[K     |█████                           | 10kB 20.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 20kB 23.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 30kB 15.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 40kB 11.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 51kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 61kB 9.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 71kB 6.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 8.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.7.0+cu101)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.19.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.8)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "  Found existing installation: torchtext 0.3.1\n",
      "    Uninstalling torchtext-0.3.1:\n",
      "      Successfully uninstalled torchtext-0.3.1\n",
      "Successfully installed sentencepiece-0.1.94 torchtext-0.6.0\n",
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.0.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Collecting de_core_news_sm==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9MB 564kB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (51.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
      "Building wheels for collected packages: de-core-news-sm\n",
      "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907057 sha256=6bf2d3b6894dc3aa3b09bda7321d5440fbedbb6f4c5f0321d50f0cbdc88a396d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4j5onigg/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
      "Successfully built de-core-news-sm\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-2.2.5\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n"
     ]
    }
   ],
   "source": [
    "# bleu score needs\n",
    "!pip install torchtext==0.6.0\n",
    "\n",
    "# spacy language model loads\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6n_xY1E0UpfI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import torchtext\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0Qk88ndVHU7"
   },
   "outputs": [],
   "source": [
    "# For deterministic results set seed\n",
    "\n",
    "SEED = 555\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70VQGvX5WrjU"
   },
   "source": [
    "**Preprocessing the data**:\n",
    "Tokenization, adding start/end tokens and lower case, data split. Creation of iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocHBgRO8Vasg"
   },
   "outputs": [],
   "source": [
    "# Load Spacy language models for tokenizing\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SshXbieUVauo"
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    # Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    # As source seq is fed in reverse order in basic enc_dec\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    # Tokenizes English text from a string into a list of strings (tokens)\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHGgqyASVaxJ"
   },
   "outputs": [],
   "source": [
    "# Field() of TorchText: Does tokenization according to fx, appends start and end tokens and lowers() case\n",
    "\n",
    "SRC = Field(tokenize = tokenize_de, init_token = '<sos>', eos_token = '<eos>', lower = True)\n",
    "TRG = Field(tokenize = tokenize_en, init_token = '<sos>', eos_token = '<eos>', lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6x9E94mhVazT",
    "outputId": "b98ad92b-3c38-476f-fb85-3f4e5ef4e299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading training.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:01<00:00, 606kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading validation.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 173kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading mmt_task1_test2016.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 165kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Split data. Assigns source as German, target as English\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X52FqmTfVa1k",
    "outputId": "17b7f7da-3c39-4dd0-a527-97e3114583e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNrb5jt7Va3v"
   },
   "outputs": [],
   "source": [
    "# Build lang vocabulary. Discard words which occur less than x(min_freq) times\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4T5Skj0AVa70",
    "outputId": "6247e6ec-c2ff-47ac-c3e1-ab742b856a6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use Cuda if available\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mOzyzVrTVa-L"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "# Create iterators to get a batch of seq ip/op. Iterator automatically handles padding sequences to same length\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), batch_size = BATCH_SIZE, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiEiHK2AZhaa"
   },
   "source": [
    "**s2s Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QRUGVjvcTAm"
   },
   "source": [
    "Encoder:\n",
    "Takes the ip seq, applies an embedding and then passes it on to a RNN (LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXxZWLmtVbAb"
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, input_dim, hid_dim, embed_dim, n_layers, dropout):\n",
    "    super().__init__()\n",
    "    self.embed = nn.Embedding(input_dim, embed_dim)\n",
    "    self.rnn = nn.LSTM(embed_dim, hid_dim, num_layers = 2, dropout = dropout)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "  def forward(self, src):\n",
    "    embedded = self.dropout(self.embed(src)) # [seq_len, bs, embed_dim]\n",
    "    outputs, (hidden, cell) = self.rnn(embedded)\n",
    "    return hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t83SphLSVbC7"
   },
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, output_dim, hid_dim, embed_dim, n_layers, dropout):\n",
    "    super().__init__()\n",
    "    self.op_dim = output_dim\n",
    "    self.embed = nn.Embedding(output_dim, embed_dim)\n",
    "    self.rnn = nn.LSTM(embed_dim, hid_dim, num_layers = 2, dropout = dropout)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.fc = nn.Linear(hid_dim, output_dim)\n",
    "    self.verb = True\n",
    "  \n",
    "  def forward(self, tgt, fin_hs, fin_cs):\n",
    "    if self.verb:\n",
    "      print(f'Target shape before unsqueeze: {tgt.shape}\\n')\n",
    "    \n",
    "    tgt = tgt.unsqueeze(0) # [1, bs]\n",
    "    embedded = self.dropout(self.embed(tgt)) # [1, bs, embed_dim]\n",
    "    outputs, (hidden, cell) = self.rnn(embedded, (fin_hs, fin_cs))\n",
    "    \n",
    "    # outputs: [seq_len, bs, n_dir*hid_dim] -> [1, bs, hid_dim]\n",
    "    # hidden, cell: [n_layers*seq_len, bs, hid_dim] -> [2, bs, hid_dim]\n",
    "\n",
    "    if self.verb:\n",
    "      print(f'Target shape: {tgt.shape}\\nEmbed shape: {embedded.shape}\\nEnc op shape: {outputs.shape}\\n')\n",
    "    self.verb = False\n",
    "\n",
    "    pred = self.fc(outputs.squeeze(0)) # [bs, output_dim]\n",
    "    return pred, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIQwB7qOgqHk"
   },
   "source": [
    "Target shape before unsqueeze: torch.Size([128])\n",
    "\n",
    "Target shape: torch.Size([1, 128])\n",
    "Embed shape: torch.Size([1, 128, 512])\n",
    "Enc op shape: torch.Size([1, 128, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdBQBA4tZZiR"
   },
   "outputs": [],
   "source": [
    "# S2S\n",
    "\n",
    "class seq2seq(nn.Module):\n",
    "  def __init__(self, enc, dec, dev):\n",
    "    super().__init__()\n",
    "\n",
    "    self.enc = enc\n",
    "    self.dec = dec\n",
    "    self.dev = dev\n",
    "    # assumption: hidden dim of enc and dec is same! No. of layers in rnn of enc and dec is same. Else think of how to sned the fin_hs and fin_cs from enc to dec\n",
    "  \n",
    "  def forward(self, src, tgt, teacher_force_ratio=0.6):\n",
    "\n",
    "    # Init the op tensor for storing predicted op\n",
    "    tgt_len = tgt.shape[0]\n",
    "    bs = tgt.shape[1]\n",
    "    output_dim = self.dec.op_dim\n",
    "    pred_seq = torch.zeros(tgt_len, bs, output_dim).to(self.dev)\n",
    "\n",
    "    enc_hs, enc_cs = self.enc(src)\n",
    "\n",
    "    # Decoder ip\n",
    "    ip = tgt[0]\n",
    "    prev_hs, prev_cs = enc_hs, enc_cs\n",
    "\n",
    "    for x in range(1, tgt_len):\n",
    "      pred, hidden, cell = self.dec(ip, prev_hs, prev_cs)\n",
    "      prev_hs = hidden\n",
    "      prev_cs = cell\n",
    "      pred_seq[x] = pred\n",
    "\n",
    "      teacher_force = random.random() < teacher_force_ratio\n",
    "\n",
    "      # get the highest predicted token from our predictions\n",
    "      top1 = pred.argmax(1) \n",
    "            \n",
    "      # if teacher forcing, use actual next token as next input if not, use predicted token\n",
    "      ip = tgt[x] if teacher_force else top1\n",
    "    \n",
    "    return pred_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJn96tjckkoG"
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7u7DQWAZZka"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = seq2seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNvSQIoBZZmn",
    "outputId": "7037b2f8-ff54-41e5-eaed-97c90c8714be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seq2seq(\n",
       "  (enc): Encoder(\n",
       "    (embed): Embedding(7855, 512)\n",
       "    (rnn): LSTM(512, 256, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (dec): Decoder(\n",
       "    (embed): Embedding(5893, 512)\n",
       "    (rnn): LSTM(512, 256, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (fc): Linear(in_features=256, out_features=5893, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init weights: initialize all weights from a uniform distribution between -0.08 and +0.08\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20jAfOIfZZou",
    "outputId": "87182520-5919-4c40-b144-305132b6dc58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 11,183,109 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KrSVjC9VZZq7"
   },
   "outputs": [],
   "source": [
    "# Set optimizer and loss fx\n",
    "# ignore the loss whenever the target token is a padding token.\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M3H8fCxIZZtJ"
   },
   "outputs": [],
   "source": [
    "# Begin train\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "  # sets mode to train\n",
    "  model.train()\n",
    "\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for i, batch in enumerate(iterator):\n",
    "    # pull the src, tgt\n",
    "    src = batch.src\n",
    "    tgt = batch.trg\n",
    "\n",
    "    # zero the grad calculated from last batch\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # send to model\n",
    "    op = model(src, tgt)\n",
    "\n",
    "    output_dim = op.shape[-1]\n",
    "    op = op[1:].view(-1, output_dim)\n",
    "    tgt = tgt[1:].view(-1)\n",
    "\n",
    "    loss = criterion(op, tgt)\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "    optimizer.step()\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "  return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1l9WTkXnZZvX"
   },
   "outputs": [],
   "source": [
    "# Begin eval\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "  model.eval()\n",
    "  epoch_loss = 0\n",
    "\n",
    "  # No grad calculation in eval\n",
    "  with torch.no_grad():\n",
    "    for i, batch in enumerate(iterator):\n",
    "      src = batch.src\n",
    "      tgt = batch.trg\n",
    "\n",
    "      # call for forward() with teacher_force = 0\n",
    "      op = model(src, tgt, 0)\n",
    "\n",
    "      output_dim = op.shape[-1]\n",
    "      op = op[1:].view(-1, output_dim)\n",
    "      tgt = tgt[1:].view(-1)\n",
    "\n",
    "      loss = criterion(op, tgt)\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiNftb1xZZxq"
   },
   "outputs": [],
   "source": [
    "# record times\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wroUndcmZZz1",
    "outputId": "91edd1cc-3ec8-4dc5-c804-88071cbe88c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target shape before unsqueeze: torch.Size([128])\n",
      "\n",
      "Target shape: torch.Size([1, 128])\n",
      "Embed shape: torch.Size([1, 128, 512])\n",
      "Enc op shape: torch.Size([1, 128, 256])\n",
      "\n",
      "Epoch: 01 | Time: 0m 29s\n",
      "\tTrain Loss: 5.169 | Train PPL: 175.676\n",
      "\tValid Loss: 4.883 | Valid PPL: 132.030\n",
      "Epoch: 02 | Time: 0m 29s\n",
      "\tTrain Loss: 4.496 | Train PPL:  89.681\n",
      "\tValid Loss: 4.846 | Valid PPL: 127.187\n",
      "Epoch: 03 | Time: 0m 29s\n",
      "\tTrain Loss: 4.218 | Train PPL:  67.906\n",
      "\tValid Loss: 4.799 | Valid PPL: 121.443\n",
      "Epoch: 04 | Time: 0m 30s\n",
      "\tTrain Loss: 4.035 | Train PPL:  56.538\n",
      "\tValid Loss: 4.737 | Valid PPL: 114.064\n",
      "Epoch: 05 | Time: 0m 30s\n",
      "\tTrain Loss: 3.917 | Train PPL:  50.244\n",
      "\tValid Loss: 4.581 | Valid PPL:  97.565\n",
      "Epoch: 06 | Time: 0m 29s\n",
      "\tTrain Loss: 3.791 | Train PPL:  44.307\n",
      "\tValid Loss: 4.722 | Valid PPL: 112.419\n",
      "Epoch: 07 | Time: 0m 29s\n",
      "\tTrain Loss: 3.656 | Train PPL:  38.694\n",
      "\tValid Loss: 4.482 | Valid PPL:  88.403\n",
      "Epoch: 08 | Time: 0m 29s\n",
      "\tTrain Loss: 3.555 | Train PPL:  34.979\n",
      "\tValid Loss: 4.376 | Valid PPL:  79.522\n",
      "Epoch: 09 | Time: 0m 29s\n",
      "\tTrain Loss: 3.422 | Train PPL:  30.627\n",
      "\tValid Loss: 4.292 | Valid PPL:  73.129\n",
      "Epoch: 10 | Time: 0m 29s\n",
      "\tTrain Loss: 3.313 | Train PPL:  27.471\n",
      "\tValid Loss: 4.221 | Valid PPL:  68.070\n"
     ]
    }
   ],
   "source": [
    "# Begin training actually\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "  start_time = time.time()\n",
    "\n",
    "  train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "  valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "  end_time = time.time()\n",
    "\n",
    "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "  if valid_loss < best_valid_loss:\n",
    "    best_valid_loss = valid_loss\n",
    "    torch.save(model.state_dict(), 'enc_dec-model.pt')\n",
    "\n",
    "  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "  print(f'\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52ItfX-Ca4x5"
   },
   "source": [
    "**Test & Eval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70iQDAl3ZZ2A",
    "outputId": "a363b3fe-b0c0-413c-efe7-a7a64c865084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 4.216 | Test PPL:  67.787 |\n"
     ]
    }
   ],
   "source": [
    "# When testing, directly load trained model and run\n",
    "# Load trained model, test set\n",
    "\n",
    "trained_model = 'enc_dec-model.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(trained_model))\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0y5HEyPxPRW8"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, src_field, tgt_field, model, device, max_len = 50):\n",
    "  # eval mode\n",
    "  model.eval()\n",
    "\n",
    "  # tokenize src if it's a string\n",
    "  if isinstance(sentence, str):\n",
    "    nlp = spacy.load('de')\n",
    "    tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "  else:\n",
    "    tokens = [token.lower() for token in sentence]\n",
    "\n",
    "  # Add <sos>, <eos>\n",
    "  tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "  # Numericalize seq\n",
    "  src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "  # make tensor of src seq\n",
    "  src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    hs, cs = model.enc(src_tensor)\n",
    "  tgt_indexes = [tgt_field.vocab.stoi[tgt_field.init_token]]\n",
    "  for i in range(max_len):\n",
    "    # each time set the last token of tgt_tensor as input to decoder. Here no teacher_force\n",
    "    # 1st time its <sos>, then prev predicted tok by decoder\n",
    "    tgt_tensor = torch.LongTensor([tgt_indexes[-1]]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      output, hs, cs = model.dec(tgt_tensor, hs, cs)\n",
    "    pred_token = output.argmax(1).item()\n",
    "    tgt_indexes.append(pred_token)\n",
    "\n",
    "    if pred_token == tgt_field.vocab.stoi[tgt_field.eos_token]:\n",
    "      break\n",
    "    \n",
    "  # Get back the predicted tgt tokens\n",
    "  tgt_tokens = [tgt_field.vocab.itos[i] for i in tgt_indexes]\n",
    "  # cut off <sos>\n",
    "  return tgt_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jPzI4W2bOwfk",
    "outputId": "067543d0-5eec-4a24-b564-8d670ee70756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['.', 'straßenszene', 'einer', 'gemälde', 'ein', 'betrachtet', 'und', 'gehweg', 'belebten', 'einem', 'auf', 'steht', 'mantel', 'blauen', 'einem', 'in', 'person', 'eine']\n",
      "actual translation = ['a', 'person', 'dressed', 'in', 'a', 'blue', 'coat', 'is', 'standing', 'in', 'on', 'a', 'busy', 'sidewalk', ',', 'studying', 'painting', 'of', 'a', 'street', 'scene', '.']\n",
      "predicted translation = ['a', 'person', 'in', 'a', 'blue', 'shirt', 'is', 'sitting', 'on', 'a', 'sidewalk', 'with', 'a', 'a', 'a', 'a', 'a', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 25\n",
    "\n",
    "src = vars(train_data.examples[example_idx])['src']\n",
    "tgt = vars(train_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'actual translation = {tgt}')\n",
    "translation = translate(src, SRC, TRG, model, device)\n",
    "print(f'predicted translation = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hKzBuEQ9abwV",
    "outputId": "f353166c-dd5d-4687-b612-5d7af00fd03f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['.', 'springen', 'nacheinander', 'die', ',', 'mädchen', 'fünf', 'mit', 'ballettklasse', 'eine']\n",
      "actual translation = ['a', 'ballet', 'class', 'of', 'five', 'girls', 'jumping', 'in', 'sequence', '.']\n",
      "predicted translation = ['a', 'group', 'of', 'children', 'in', 'in', 'a', '.', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 10\n",
    "\n",
    "src = vars(train_data.examples[example_idx])['src']\n",
    "tgt = vars(train_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'actual translation = {tgt}')\n",
    "translation = translate(src, SRC, TRG, model, device)\n",
    "print(f'predicted translation = {translation}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bD_rNQzwablE",
    "outputId": "c935836c-c1f0-494e-c5ee-bc51ba1a2067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src = ['.', 'lächelt', 'und', 'an', 'etwas', 'blickt', 'jacke', 'schwarz-gelben', 'einer', 'in', 'mann', 'junger', 'ein']\n",
      "actual translation = ['a', 'young', 'man', 'in', 'a', 'black', 'and', 'yellow', 'jacket', 'is', 'gazing', 'at', 'something', 'and', 'smiling', '.']\n",
      "predicted translation = ['a', 'young', 'man', 'in', 'a', 'black', 'shirt', 'and', 'holding', 'a', '<unk>', 'of', 'her', '.', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "example_idx = 29\n",
    "\n",
    "src = vars(train_data.examples[example_idx])['src']\n",
    "tgt = vars(train_data.examples[example_idx])['trg']\n",
    "\n",
    "print(f'src = {src}')\n",
    "print(f'actual translation = {tgt}')\n",
    "translation = translate(src, SRC, TRG, model, device)\n",
    "print(f'predicted translation = {translation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k316o1GnZ_tL"
   },
   "source": [
    "**From some examples, seems like the model is able to transalte the first few words of the sentence, but later parts are completely missed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyP3L-dDazIx"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "  trgs = []\n",
    "  pred_trgs = []\n",
    "\n",
    "  for datum in data:\n",
    "    src = vars(datum)['src']\n",
    "    trg = vars(datum)['trg']\n",
    "    pred_trg = translate(src, src_field, trg_field, model, device, max_len)\n",
    "    \n",
    "    # cut off <eos>\n",
    "    pred_trg = pred_trg[:-1]\n",
    "    \n",
    "    pred_trgs.append(pred_trg)\n",
    "    trgs.append([trg])\n",
    "      \n",
    "  return bleu_score(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31iBcegFazRd",
    "outputId": "b96512b5-ccd5-4dce-95ae-cab5850be081"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score w simple enc_dec = 8.52\n"
     ]
    }
   ],
   "source": [
    "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "\n",
    "print(f'BLEU score w simple enc_dec = {bleu_score*100:.2f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "enc_dec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}